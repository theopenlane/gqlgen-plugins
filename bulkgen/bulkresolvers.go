package bulkgen

import (
	_ "embed"
	"encoding/csv"
	"encoding/json"
	"fmt"
	"os"
	"strings"
	"text/template"

	"github.com/99designs/gqlgen/codegen"
	"github.com/99designs/gqlgen/codegen/templates"
	"github.com/99designs/gqlgen/plugin"
	"github.com/gertd/go-pluralize"
	"github.com/rs/zerolog/log"
	"github.com/stoewer/go-strcase"
)

//go:embed bulk.gotpl
var bulkTemplate string

// New returns a new plugin
func New() plugin.Plugin {
	return &Plugin{}
}

// Options is a function to set the options for the plugin
type Options func(*Plugin)

// NewWithOptions returns a new plugin with the given options
func NewWithOptions(opts ...Options) *Plugin {
	r := &Plugin{}

	for _, opt := range opts {
		opt(r)
	}

	return r
}

// WithModelPackage sets the model package for the gqlgen model
func WithModelPackage(modelPackage string) Options {
	return func(p *Plugin) {
		p.ModelPackage = modelPackage
	}
}

// WithEntGeneratedPackage sets the ent generated package for the gqlgen model
func WithEntGeneratedPackage(entPackage string) Options {
	return func(p *Plugin) {
		p.EntGeneratedPackage = entPackage
	}
}

// WithGraphQLImport sets the import path for the graphql package
func WithGraphQLImport(graphqlImport string) Options {
	return func(p *Plugin) {
		p.GraphQLImport = graphqlImport
	}
}

// WithCSVOutputPath sets the file path location that CSVs are written to
func WithCSVOutputPath(path string) Options {
	return func(p *Plugin) {
		p.CSVOutputPath = path
	}
}

// WithCSVGeneratedPackage sets the import path for the csvgenerated package
func WithCSVGeneratedPackage(pkg string) Options {
	return func(p *Plugin) {
		p.CSVGeneratedPackage = pkg
	}
}

// WithCSVFieldMappingsFile sets the path to the JSON file containing CSV field mappings.
// This file is generated by entx and contains custom CSV column definitions.
func WithCSVFieldMappingsFile(path string) Options {
	return func(p *Plugin) {
		p.CSVFieldMappingsFile = path
	}
}

// Plugin is a gqlgen plugin to generate bulk resolver functions used for mutations
type Plugin struct {
	// ModelPackage is the package name for the gqlgen model
	ModelPackage string
	// EntGeneratedPackage is the ent generated package that holds the generated types
	EntGeneratedPackage string
	// GraphQLImport is the import path for the graphql package
	GraphQLImport string
	// CSVOutputPath is the file path location that CSVs are written to
	CSVOutputPath string
	// CSVGeneratedPackage is the import path for the csvgenerated package
	CSVGeneratedPackage string
	// CSVFieldMappingsFile is the path to the JSON file containing CSV field mappings
	CSVFieldMappingsFile string
}

// Name returns the name of the plugin
func (m *Plugin) Name() string {
	return "bulkgen"
}

// BulkResolverBuild is a struct to hold the objects for the bulk resolver
type BulkResolverBuild struct {
	// Objects is a list of objects to generate bulk resolvers for
	Objects []Object
	// ModelImport is the import path for the gqlgen model
	ModelImport string
	// EntImport is the ent generated package that holds the generated types
	EntImport string
	// GraphQLImport is the import path for the graphql package
	GraphQLImport string
	// ModelPackage is the package name for the gqlgen model
	ModelPackage string
	// CSVGeneratedImport is the import path for the csvgenerated package
	CSVGeneratedImport string
}

// Object is a struct to hold the object name for the bulk resolver
type Object struct {
	// Name of the object
	Name string
	// PluralName of the object
	PluralName string
	// Fields of the object
	Fields []string
	// AppendFields is the list of fields that can be appended in the update mutation
	AppendFields []string
	// OperationType indicates whether this is a create or delete operation
	OperationType string
	// HasCSVUpdateMutation indicates if this object has a CSV bulk update mutation
	HasCSVUpdateMutation bool
	// CSVFieldMappings contains custom CSV column mappings for this object
	CSVFieldMappings []CSVFieldMapping
}

// CSVFieldMapping represents a custom CSV column that maps to a field.
// This structure matches the JSON format generated by entx.
type CSVFieldMapping struct {
	// CSVColumn is the friendly CSV header name (e.g., AssignedToUserEmail)
	CSVColumn string `json:"csvColumn"`
	// TargetField is the Go field name this column maps to (e.g., AssignedToUserID)
	TargetField string `json:"targetField"`
	// IsSlice indicates if the field is a []string
	IsSlice bool `json:"isSlice"`
}

// CSVFieldMappingsJSON is the top-level structure for the JSON mappings file.
// Maps schema names to their CSV field mappings.
type CSVFieldMappingsJSON map[string][]CSVFieldMapping

// GenerateCode generates the bulk resolver code
func (m *Plugin) GenerateCode(data *codegen.Data) error {
	if !data.Config.Resolver.IsDefined() {
		return nil
	}

	return m.generateSingleFile(*data)
}

// loadCSVFieldMappings reads the CSV field mappings from the JSON file.
// Returns an empty map if the file doesn't exist or can't be read.
func loadCSVFieldMappings(filePath string) CSVFieldMappingsJSON {
	if filePath == "" {
		return nil
	}

	file, err := os.Open(filePath)
	if err != nil {
		log.Debug().Err(err).Str("path", filePath).Msg("CSV field mappings file not found, skipping custom columns")
		return nil
	}

	defer file.Close()

	var mappings CSVFieldMappingsJSON
	if err := json.NewDecoder(file).Decode(&mappings); err != nil {
		log.Warn().Err(err).Str("path", filePath).Msg("failed to decode CSV field mappings JSON")
		return nil
	}

	log.Debug().Str("path", filePath).Int("schemas", len(mappings)).Msg("loaded CSV field mappings")

	return mappings
}

// generateSingleFile generates the bulk resolver code, this is all done in a single file and
// used by the resolvergen plugin for each bulk resolver
func (m *Plugin) generateSingleFile(data codegen.Data) error {
	inputData := BulkResolverBuild{
		Objects:            []Object{},
		ModelImport:        m.ModelPackage,
		EntImport:          m.EntGeneratedPackage,
		GraphQLImport:      m.GraphQLImport,
		CSVGeneratedImport: m.CSVGeneratedPackage,
	}

	// Build a set of object names that have CSV bulk mutations.
	// Uses case-insensitive prefix matching to handle naming variations,
	// while only matching mutations that start with expected prefixes to avoid
	// false positives with custom resolvers containing these substrings.
	csvBulkMutations := make(map[string]bool)

	for _, f := range data.Schema.Mutation.Fields {
		if objectName := extractObjectNameFromCSVMutation(f.Name); objectName != "" {
			csvBulkMutations[objectName] = true
		}
	}

	// only add the model package if the import is not empty
	if m.ModelPackage != "" {
		modelPkg := data.Config.Model.Package
		if modelPkg != "" {
			modelPkg += "."
		}

		inputData.ModelPackage = modelPkg
	}

	if m.CSVOutputPath == "" {
		m.CSVOutputPath = data.Config.Resolver.Dir() + "/csv"
	}

	// create the directory if it does not exist
	if _, err := os.Stat(m.CSVOutputPath); os.IsNotExist(err) {
		if err := os.MkdirAll(m.CSVOutputPath, os.ModePerm); err != nil {
			return err
		}
	}

	// Load CSV field mappings from JSON file if configured
	csvFieldMappings := loadCSVFieldMappings(m.CSVFieldMappingsFile)

	for _, f := range data.Schema.Mutation.Fields {
		lowerName := strings.ToLower(f.Name)

		// if the field is a bulk create or delete mutation, add it to the list of objects
		// we skip csv bulk mutations because they will reuse the same functions
		if strings.Contains(lowerName, "bulk") && !strings.Contains(lowerName, "csv") {
			var objectName, operationType string

			switch {
			case strings.Contains(lowerName, "createbulk"):
				objectName = strings.Replace(f.Name, "createBulk", "", 1)
				operationType = "create"
			case strings.Contains(lowerName, "bulkcreate"):
				objectName = strings.Replace(f.Name, "bulkCreate", "", 1)
				operationType = "create"
			case strings.Contains(lowerName, "updatebulk"):
				objectName = strings.Replace(f.Name, "updateBulk", "", 1)
				operationType = "update"
			case strings.Contains(lowerName, "bulkupdate"):
				objectName = strings.Replace(f.Name, "bulkUpdate", "", 1)
				operationType = "update"
			case strings.Contains(lowerName, "deletebulk"):
				objectName = strings.Replace(f.Name, "deleteBulk", "", 1)
				operationType = "delete"
			case strings.Contains(lowerName, "bulkdelete"):
				objectName = strings.Replace(f.Name, "bulkDelete", "", 1)
				operationType = "delete"
			default:
				continue
			}

			object := Object{
				Name:                 objectName,
				PluralName:           pluralFieldName(objectName),
				Fields:               getCreateInputFields(objectName, data),
				AppendFields:         getUpdateAppendFields(objectName, data),
				OperationType:        operationType,
				HasCSVUpdateMutation: csvBulkMutations[objectName],
				CSVFieldMappings:     csvFieldMappings[objectName],
			}

			inputData.Objects = append(inputData.Objects, object)

			// Generate and write the CSV file only for create operations
			if operationType == "create" {
				if err := generateSampleCSV(object, m.CSVOutputPath); err != nil {
					return err
				}
			}
		}
	}

	// render the bulk resolver template
	return templates.Render(templates.Options{
		PackageName: data.Config.Resolver.Package,            // use the resolver package
		Filename:    data.Config.Resolver.Dir() + "/bulk.go", // write to the resolver directory
		FileNotice:  `// THIS CODE IS REGENERATED BY github.com/theopenlane/core/pkg/gqlplugin. DO NOT EDIT.`,
		Data:        inputData,
		Funcs: template.FuncMap{
			"toLower": strings.ToLower,
		},
		Packages: data.Config.Packages,
		Template: bulkTemplate,
	})
}

// getCreateInputFields returns the list of fields available in the Create<object>Input
func getCreateInputFields(objectName string, data codegen.Data) (inputFields []string) {
	inputTypeName := "Create" + objectName + "Input"
	if inputType, ok := data.Schema.Types[inputTypeName]; ok {
		for _, f := range inputType.Fields {
			inputFields = append(inputFields, strcase.UpperCamelCase(f.Name))
		}
	}

	return inputFields
}

// getUpdateAppendFields returns the list of fields that are appendable in the bulk update mutation
func getUpdateAppendFields(objectName string, data codegen.Data) (appendFields []string) {
	inputTypeName := "Update" + objectName + "Input"
	if inputType, ok := data.Schema.Types[inputTypeName]; ok {
		for _, f := range inputType.Fields {
			if strings.Contains(f.Name, "append") {
				appendFields = append(appendFields, templates.ToGo(f.Name))
			}
		}
	}

	return appendFields
}

// generateSampleCSV generates a sample CSV file for the given object.
// It includes both standard input fields and custom CSV column mappings from entx annotations.
func generateSampleCSV(object Object, outputPath string) error {
	headers := make([]string, 0, len(object.Fields)+len(object.CSVFieldMappings))
	headers = append(headers, object.Fields...)

	for _, mapping := range object.CSVFieldMappings {
		headers = append(headers, mapping.CSVColumn)
	}

	filePath := fmt.Sprintf("%s/sample_%s.csv", outputPath, strings.ToLower(object.Name))

	file, err := os.Create(filePath)
	if err != nil {
		return err
	}

	defer file.Close()

	writer := csv.NewWriter(file)

	defer writer.Flush()

	if err := writer.Write(headers); err != nil {
		return err
	}

	exampleRow := make([]string, len(headers))

	for i := range object.Fields {
		exampleRow[i] = fmt.Sprintf("example_%s", strings.ToLower(object.Fields[i]))
	}

	for i, mapping := range object.CSVFieldMappings {
		idx := len(object.Fields) + i

		if mapping.IsSlice {
			exampleRow[idx] = fmt.Sprintf("example_%s1,example_%s2", strings.ToLower(mapping.CSVColumn), strings.ToLower(mapping.CSVColumn))
		} else {
			exampleRow[idx] = fmt.Sprintf("example_%s", strings.ToLower(mapping.CSVColumn))
		}
	}

	if err := writer.Write(exampleRow); err != nil {
		return err
	}

	log.Debug().Str("object", object.Name).Str("path", filePath).Int("customColumns", len(object.CSVFieldMappings)).Msg("sample CSV created")

	return nil
}

// csvMutationPrefixes defines the supported prefixes for CSV bulk mutations.
// Each entry maps a lowercase prefix to its canonical cased version for extraction.
var csvMutationPrefixes = []struct {
	lowerPrefix string
	len         int
}{
	{"updatebulkcsv", 13},
	{"bulkcsvupdate", 13},
	{"csvbulkupdate", 13},
	{"createbulkcsv", 13},
	{"bulkcsvcreate", 13},
	{"csvbulkcreate", 13},
}

// extractObjectNameFromCSVMutation extracts the object name from a CSV bulk mutation name.
// Uses case-insensitive prefix matching to handle variations like createBulkCSV, CreateBulkCsv, etc.
// Only matches mutations that start with the expected prefix to avoid false positives with
// custom resolvers that happen to contain these substrings.
// Examples: updateBulkCSVControl -> Control, createBulkCSVPolicy -> Policy
func extractObjectNameFromCSVMutation(mutationName string) string {
	lowerName := strings.ToLower(mutationName)

	for _, p := range csvMutationPrefixes {
		if strings.HasPrefix(lowerName, p.lowerPrefix) {
			return mutationName[p.len:]
		}
	}

	return ""
}

// pluralFieldName makes go-pluralize and gqlgen return same values for certain edgecases
//
// go-pluralize keeps character casing, so TrustCenterFAQ would be "TrustCenterFAQS"
// gqlgen generates it as "TrustCenterFAQs".
//
// this normalizes it to match
func pluralFieldName(name string) string {
	snakeCase := strcase.SnakeCase(name)
	pluralizedValue := pluralize.NewClient().Plural(snakeCase)

	if strings.HasPrefix(pluralizedValue, snakeCase) {
		return name + pluralizedValue[len(snakeCase):]
	}

	return pluralize.NewClient().Plural(name)
}
